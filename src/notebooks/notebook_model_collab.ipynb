{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM-CNN Model for ASL\n",
        "\n",
        "This notebook implements a collaborative pipeline for data acquisition, preprocessing, model definition, training, and evaluation to recognize American Sign Language (ASL) gestures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install necessary packages and mount Google Drive and import all required libraries and frameworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl0syz3jVGlI",
        "outputId": "70d1763a-0eb2-4b9f-c537-541d0af74231"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua-88wvnUzvC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import zipfile\n",
        "import shutil\n",
        "import random as rnd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from copy import deepcopy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Acquisition\n",
        "\n",
        "Download and extract the dataset from Kaggle (for google collab development, place kaggle.json in google drive).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "oWDdlXwKXhxR",
        "outputId": "4ffce828-7251-425d-b1d7-c515f43b07eb"
      },
      "outputs": [],
      "source": [
        "# Configure Kaggle credentials and extract the processed WLASL dataset\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "shutil.move(\"root/.kaggle/kaggle.json\", \"drive/MyDrive/kaggle.json\")\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnmTAUKLUzvE"
      },
      "outputs": [],
      "source": [
        "# Extract dataset\n",
        "with zipfile.ZipFile(\"artifact/wlasl-processed.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"artifact/wlasl-processed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Processing\n",
        "\n",
        "Video pre-processing and Frame Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E_zeMlIHUzvF"
      },
      "outputs": [],
      "source": [
        "def videos_process(output_folder: str, json_fp: str, videos_folder: str): \n",
        "    \"\"\"\n",
        "    Copy and organize raw video files into train/val/test splits.\n",
        "\n",
        "    Args:\n",
        "        output_folder: top‐level folder to receive split directories.\n",
        "        json_fp: path to WLASL_v0.3.json metadata file.\n",
        "        videos_folder: folder containing original .mp4 files.\n",
        "    \"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    # Opening the json\n",
        "    with open(json_fp, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "\n",
        "    for gloss_data in data:\n",
        "        gloss_name = gloss_data[\"gloss\"]\n",
        "\n",
        "        for instance in gloss_data[\"instances\"]:\n",
        "            video_id = instance[\"video_id\"]\n",
        "            split = instance[\"split\"] # 'train', 'val', or 'test'\n",
        "\n",
        "\n",
        "            source_path = os.path.join(videos_folder, f\"{video_id}.mp4\")\n",
        "            dest_folder = os.path.join(output_folder, split, gloss_name)\n",
        "            dest_path = os.path.join(dest_folder, f\"{video_id}.mp4\")\n",
        "\n",
        "            # Ensuring that destination folder exists\n",
        "            os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "            if os.path.exists(source_path):\n",
        "                shutil.copy(source_path, dest_path)\n",
        "                print(f\"Copied {source_path} to {dest_path}\")\n",
        "            else:\n",
        "                print(f\"Video not found: {source_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoNvSZ2rUzvG"
      },
      "outputs": [],
      "source": [
        "def bbox_file(info: str, output_json_path: str):\n",
        "    \"\"\"\n",
        "    Build a lookup from video_id to bounding box.\n",
        "\n",
        "    Args:\n",
        "        info: path to WLASL_v0.3.json file with bbox info per instance.\n",
        "        output_json_path: where to write the simplified {video_id: bbox} map.\n",
        "    \"\"\"\n",
        "    with open(info, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    id_to_bbox = {}\n",
        "    for gloss_data in data:\n",
        "        for instance in gloss_data[\"instances\"]:\n",
        "            video_id = instance[\"video_id\"]\n",
        "            boundbox = instance[\"bbox\"]\n",
        "            id_to_bbox[video_id] = boundbox\n",
        "\n",
        "\n",
        "    with open(output_json_path, \"w\") as file:\n",
        "        json.dump(id_to_bbox, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaaHUgjSUzvG"
      },
      "outputs": [],
      "source": [
        "def obtain_bbox(path_to_file: str, name: str) -> list:\n",
        "    \"\"\"\n",
        "    Retrieve the bounding box for a given video.\n",
        "\n",
        "    Args:\n",
        "        path_to_file: JSON file mapping IDs→bboxes.\n",
        "        name: video_id string.\n",
        "    Returns:\n",
        "        List of four ints [x_min, y_min, x_max, y_max].\n",
        "    \"\"\"\n",
        "    with open(path_to_file, 'r') as file:\n",
        "        bboxs = json.load(file)\n",
        "        return bboxs[name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtOTXBDdUzvG"
      },
      "outputs": [],
      "source": [
        "def crop_frame(bbox: list, frame: np.ndarray):\n",
        "    \"\"\"\n",
        "    Crop a video frame to the specified bounding box.\n",
        "\n",
        "    Args:\n",
        "        bbox: [x_min, y_min, x_max, y_max]\n",
        "        frame: H×W×C image array.\n",
        "    Returns:\n",
        "        Cropped frame.\n",
        "    \"\"\"\n",
        "    cropped_frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
        "    return cropped_frame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmofCWLyUzvG"
      },
      "outputs": [],
      "source": [
        "def capture_frames(video_path: str, output_file: str, bbox: list = None):\n",
        "    \"\"\"\n",
        "    Extract and save a sequence of frames from a video.\n",
        "\n",
        "    Args:\n",
        "        video_path: path to .mp4 file.\n",
        "        output_file: directory to save extracted .jpg frames.\n",
        "        bbox: optional bounding box to crop each frame.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Optionally crop to bbox region\n",
        "        if bbox:\n",
        "            frame = crop_frame(bbox, frame)\n",
        "\n",
        "        # Save only frames 5 through 35 (inclusive)\n",
        "        if frame_count >= 5 and frame_count <= 35:\n",
        "            frame_filename = f\"video{os.path.basename(video_path).replace('.mp4', '')}_frame_{frame_count}.jpg\"\n",
        "            cv2.imwrite(os.path.join(output_file, frame_filename), frame)\n",
        "        frame_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0H0iLwpUzvG"
      },
      "outputs": [],
      "source": [
        "bbox_path = \"drive/MyDrive/newfile.json\"\n",
        "videosfilepath = \"drive/MyDrive/dataset_split\"\n",
        "output_folder = \"drive/MyDrive/frames\"\n",
        "\n",
        "splits = ['test','train','val']\n",
        "\n",
        "\n",
        "def extract_and_crop_all_splits(\n",
        "    videos_base: str,\n",
        "    frames_output: str,\n",
        "    bbox_json: str,\n",
        "    splits: list[str] = ('train', 'val', 'test')\n",
        "):\n",
        "    \"\"\"\n",
        "    Walk through each split/class subfolder, crop & extract frames per video.\n",
        "\n",
        "    Args:\n",
        "        videos_base: root folder containing split subfolders (train/val/test).\n",
        "        frames_output: root folder where per-split frames will be saved.\n",
        "        bbox_json: path to JSON mapping video_id -> [x_min,y_min,x_max,y_max].\n",
        "        splits: list of split names (defaults to ['train','val','test']).\n",
        "    \"\"\"\n",
        "    os.makedirs(frames_output, exist_ok=True)\n",
        "    for split in splits:\n",
        "\n",
        "        files = [f for f in pathlib.Path(os.path.join(videos_base,split)).iterdir()]\n",
        "        for file in files:\n",
        "            i = 0\n",
        "            word = os.path.basename(file)\n",
        "\n",
        "            output_file = os.path.join(frames_output, split, word)\n",
        "\n",
        "            os.makedirs(output_file, exist_ok=True)\n",
        "\n",
        "            if file.is_dir():\n",
        "                videos = [f for f in file.iterdir()]\n",
        "                \n",
        "            else:\n",
        "                print(f\"Skipping non-directory: {file}\")\n",
        "                continue\n",
        "\n",
        "            for video_path in videos:\n",
        "\n",
        "                video_id = os.path.basename(video_path).replace(\".mp4\", \"\" )\n",
        "                print(video_id, video_path)\n",
        "                bbox = obtain_bbox(bbox_json, video_id)\n",
        "\n",
        "                try:\n",
        "                    vid_output = os.path.join(output_file, f\"{i}\")\n",
        "                    os.makedirs(vid_output, exist_ok=True)\n",
        "\n",
        "                    capture_frames(video_path, vid_output, bbox)\n",
        "                except Exception as e:\n",
        "                    print(bbox)\n",
        "\n",
        "                i += 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NSEARK5UzvG"
      },
      "outputs": [],
      "source": [
        "class FrameSequencer(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset that returns fixed‐length sequences of image frames.\n",
        "\n",
        "    Args:\n",
        "        root_dir: top‐level folder containing per-class subdirectories.\n",
        "        sample_size: number of frames per sequence.\n",
        "        batch_size: for __len__ calculation (unused in __getitem__).\n",
        "        target_size: (H, W) to resize each frame.\n",
        "        shuffle: whether to randomize sample order.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, sample_size = 5, batch_size = 35, target_size=(224, 224), shuffle=True):\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.sample_size = sample_size\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Gather class names and assign integer labels\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_indicies = {clss: i for i, clss in enumerate(self.classes)}\n",
        "\n",
        "        # Build (sequence_paths, label) list\n",
        "        self.samples = []\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            vids = os.listdir(cls_dir)\n",
        "            for i in vids:\n",
        "                frame_files = (sorted(\n",
        "                    [os.path.join(cls_dir, i, fname)\n",
        "                                    for fname in os.listdir(os.path.join(cls_dir, i)) if fname.lower().endswith('.jpg')]\n",
        "                                    ))\n",
        "\n",
        "                # Slide window of length sample_size\n",
        "                for j in range(0, len(frame_files) - self.sample_size + 1, self.sample_size):\n",
        "                    sequence = frame_files[j: j + self.sample_size]\n",
        "                    self.samples.append((sequence, self.class_indicies[cls]))\n",
        "\n",
        "\n",
        "        self.shuffler()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of batches (ceil of total samples / batch_size)\n",
        "        return int(np.ceil(len(self.samples) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        seq_paths, label = self.samples[index]\n",
        "        sequence_imgs = []\n",
        "\n",
        "        for fp in seq_paths:\n",
        "\n",
        "            img = cv2.imread(fp)\n",
        "            if img is None:\n",
        "                continue\n",
        "            img = cv2.resize(img, self.target_size)\n",
        "\n",
        "            # Convert H×W×C → C×H×W\n",
        "            img = np.transpose(img, (2, 0, 1))\n",
        "            sequence_imgs.append(img)\n",
        "\n",
        "        # Stack and convert to torch tensors\n",
        "        images = torch.from_numpy(np.array(sequence_imgs)).float()\n",
        "        label = torch.tensor(label).long()\n",
        "\n",
        "        return images, label\n",
        "\n",
        "\n",
        "    def shuffler(self):\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(self.samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeiX-yQ6UzvH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_sample(g):\n",
        "        \"\"\"\n",
        "        Visualize the first batch of sequences from a FrameSequencer.\n",
        "        \"\"\"\n",
        "        images, labels = g[0]\n",
        "\n",
        "        for i, sequence in enumerate(images):\n",
        "            # Find class name from label\n",
        "            classname = list(g.class_indicies.keys())[list(g.class_indicies.values()).index(labels[i])]\n",
        "            rows = 1\n",
        "            cols = len(sequence)\n",
        "            for j, image in enumerate(sequence):\n",
        "\n",
        "\n",
        "                plt.subplot(rows, cols, j + 1)\n",
        "                # Convert BGR→RGB for display\n",
        "                plt.imshow(cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2RGB))\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(classname)\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYiC9FycUzvH"
      },
      "outputs": [],
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN+LSTM architecture:\n",
        "      - Pretrained MobileNetV2 for spatial feature extraction.\n",
        "      - LSTM to capture temporal dependencies.\n",
        "      - Final Linear layer for classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_size=1280, hidden_size=256, num_classes=10, num_layers=1, device=\"cpu\"):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "        self.cnn = mobilenet.features.to(device)\n",
        "        self.lstm = nn.LSTM(input_size=feature_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: tensor of shape (batch, seq_len, C, H, W)\n",
        "        Returns logits for each class.\n",
        "        \"\"\"        \n",
        "\n",
        "        batch_size, seq_len, C, H, W = x.shape\n",
        "        # Merge batch and sequence dims for CNN\n",
        "        x = x.view(batch_size * seq_len, C, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = self.cnn(x) # (batch*seq, feat_map, h, w)\n",
        "            features = F.adaptive_avg_pool2d(features, (1, 1)) # (batch*seq, feat_map, 1, 1)\n",
        "\n",
        "        lstm_input = features.view(batch_size, seq_len, -1)\n",
        "\n",
        "        output, _ = self.lstm(lstm_input)\n",
        "        last_hidden = output[:, -1, :]  # final hidden state\n",
        "        return self.fc(last_hidden)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training Utilities\n",
        "\n",
        "- `train_one_epoch`:  \n",
        "  Trains the model over one pass of `train_loader`, returns avg. loss.\n",
        "\n",
        "- `validate`:  \n",
        "  Evaluates on `val_loader` without gradient updates, returns avg. loss.\n",
        "\n",
        "- `EarlyStopping`:  \n",
        "  Stops training if validation loss doesn’t improve for `patience` epochs,\n",
        "  and optionally restores the best-performing weights.\n",
        "\n",
        "- `training_loop`:  \n",
        "  Combines all the above into a multi-epoch loop with progress logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft7P5ow6Phde"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "        \"\"\"\n",
        "        Initialize the EarlyStopping object.\n",
        "\n",
        "        Args:\n",
        "            patience (int, optional): How many epochs to wait after last improvement.\n",
        "                Defaults to 5.\n",
        "            min_delta (float, optional): Minimum decrease in loss to qualify as improvement.\n",
        "                Defaults to 0.\n",
        "            restore_best_weights (bool, optional): If True, the model will be restored\n",
        "                to the state with the lowest validation loss. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "\n",
        "    def __call__(self, model, validation_loss):\n",
        "        \"\"\"\n",
        "        Check if validation loss has improved and update internal state.\n",
        "\n",
        "        This method should be called at the end of each epoch.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The model being trained.\n",
        "            validation_loss (float): The loss computed on the validation set for the current epoch.\n",
        "        \"\"\"\n",
        "        if validation_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = validation_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_model_state = deepcopy(model.state_dict())\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def restore_model(self, model):\n",
        "        \"\"\"\n",
        "        Restore the model's weights to the best observed state.\n",
        "\n",
        "        Args:\n",
        "            model (torch.nn.Module): The model instance to restore.\n",
        "\n",
        "        Returns:\n",
        "            torch.nn.Module: The model with weights reset to the best observed validation performance.\n",
        "        \"\"\"\n",
        "        if self.best_model_state is not None:\n",
        "            model.load_state_dict(self.best_model_state)\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rORXgDVQQCpL"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Evaluate the model on a validation dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network to evaluate.\n",
        "        dataloader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
        "        criterion (callable): Loss function to compute the validation loss.\n",
        "        device (str or torch.device, optional): Device on which to run the computations.\n",
        "            Defaults to \"cpu\".\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss over the entire validation set.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Train the model for a single epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network to train.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
        "        criterion (callable): Loss function to compute training loss.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameter updates.\n",
        "        device (str or torch.device, optional): Device on which to run the computations.\n",
        "            Defaults to \"cpu\".\n",
        "\n",
        "    Returns:\n",
        "        float: The average training loss over all batches in this epoch.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def training_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    early_stopper=None,\n",
        "    epochs=10,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Run the full training and validation loop for multiple epochs, with optional early stopping.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The neural network to train and validate.\n",
        "        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
        "        val_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n",
        "        criterion (callable): Loss function to compute training and validation losses.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer for model parameter updates.\n",
        "        early_stopper (callable, optional): An early stopping object with attributes\n",
        "            `early_stop` (bool) and `restore_best_weights` (bool), and methods\n",
        "            `__call__(model, val_loss)` and `restore_model(model)`. Defaults to None.\n",
        "        epochs (int, optional): Maximum number of epochs to run. Defaults to 10.\n",
        "        device (str or torch.device, optional): Device on which to run the computations.\n",
        "            Defaults to \"cpu\".\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The trained model. If early stopping is used and `restore_best_weights`\n",
        "        is True, returns the model restored to the best observed validation performance.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopper:\n",
        "            early_stopper(model, val_loss)\n",
        "            if early_stopper.early_stop:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Restore best model if early stopping was used\n",
        "    if early_stopper and early_stopper.restore_best_weights:\n",
        "        model = early_stopper.restore_model(model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Everything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_file_path = \"artifact/wlasl-processed/WLASL_v0.3.json\"\n",
        "videos_folder = \"artifact/wlasl-processed/videos\"\n",
        "output_folder = \"drive/MyDrive/dataset_split\"\n",
        "\n",
        "videos_process(output_folder, json_file_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_file_path = \"artifact/wlasl-processed/WLASL_v0.3.json\"\n",
        "newjson_file_path = \"drive/MyDrive/newfile.json\"\n",
        "\n",
        "bbox_file(json_file_path, newjson_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Dataloader & Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_M9pi9vqJ17"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2,\n",
        "                            pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, num_workers=2,\n",
        "                          pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NvdIEr1OGYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/train\", batch_size=5, sample_size=5)\n",
        "val_dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/val\", batch_size=5, sample_size=5)\n",
        "test_dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/test\", batch_size=5, sample_size=5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Model and Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOQxJNHzKJlC"
      },
      "outputs": [],
      "source": [
        "model = CNN_LSTM(num_classes=len(dataset.classes), device=\"cuda\").to(\"cuda\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwBiQSC7XbGs"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Running Training & Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgKPT33CazrL",
        "outputId": "bd65ae00-227e-4e6b-9c5d-d812de7e21fd"
      },
      "outputs": [],
      "source": [
        "model = training_loop(\n",
        "    model=model,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    early_stopper=early_stopping,\n",
        "    epochs=10,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOEOfciUbaSw"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"asl_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
