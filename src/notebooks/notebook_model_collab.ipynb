{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM-CNN Model for ASL\n",
        "\n",
        "This notebook implements a collaborative pipeline for data acquisition, preprocessing, model definition, training, and evaluation to recognize American Sign Language (ASL) gestures.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Install necessary packages and mount Google Drive and import all required libraries and frameworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl0syz3jVGlI",
        "outputId": "70d1763a-0eb2-4b9f-c537-541d0af74231"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua-88wvnUzvC"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import zipfile\n",
        "import shutil\n",
        "import random as rnd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import secrets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Acquisition\n",
        "\n",
        "Download and extract the dataset from Kaggle (for google collab development, place kaggle.json in google drive).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "oWDdlXwKXhxR",
        "outputId": "4ffce828-7251-425d-b1d7-c515f43b07eb"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "shutil.move(\"root/.kaggle/kaggle.json\", \"drive/MyDrive/kaggle.json\")\n",
        "\n",
        "os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnmTAUKLUzvE"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"artifact/wlasl-processed.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"artifact/wlasl-processed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E_zeMlIHUzvF"
      },
      "outputs": [],
      "source": [
        "def videos_process(output_folder, json_fp, videos_folder): \n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    # Opening the json\n",
        "    with open(json_fp, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "\n",
        "    for gloss_data in data:\n",
        "        gloss_name = gloss_data[\"gloss\"]\n",
        "\n",
        "        for instance in gloss_data[\"instances\"]:\n",
        "            video_id = instance[\"video_id\"]\n",
        "            split = instance[\"split\"]\n",
        "\n",
        "\n",
        "            source_path = os.path.join(videos_folder, f\"{video_id}.mp4\")\n",
        "            dest_folder = os.path.join(output_folder, split, gloss_name)\n",
        "            dest_path = os.path.join(dest_folder, f\"{video_id}.mp4\")\n",
        "\n",
        "            # Ensuring that destination folder exists\n",
        "            os.makedirs(dest_folder, exist_ok=True)\n",
        "\n",
        "            if os.path.exists(source_path):\n",
        "                shutil.copy(source_path, dest_path)\n",
        "                print(f\"Copied {source_path} to {dest_path}\")\n",
        "            else:\n",
        "                print(f\"Video not found: {source_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json_file_path = \"artifact/wlasl-processed/WLASL_v0.3.json\"\n",
        "videos_folder = \"artifact/wlasl-processed/videos\"\n",
        "output_folder = \"drive/MyDrive/dataset_split\"\n",
        "\n",
        "videos_process(output_folder, json_file_path, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoNvSZ2rUzvG"
      },
      "outputs": [],
      "source": [
        "def bbox_file(info, output_json_path):\n",
        "    with open(info, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    id_to_bbox = {}\n",
        "    for gloss_data in data:\n",
        "        for instance in gloss_data[\"instances\"]:\n",
        "            video_id = instance[\"video_id\"]\n",
        "            boundbox = instance[\"bbox\"]\n",
        "            id_to_bbox[video_id] = boundbox\n",
        "\n",
        "\n",
        "    with open(output_json_path, \"w\") as file:\n",
        "        json.dump(id_to_bbox, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWMGlkiCkX9M"
      },
      "outputs": [],
      "source": [
        "json_file_path = \"artifact/wlasl-processed/WLASL_v0.3.json\"\n",
        "newjson_file_path = \"drive/MyDrive/newfile.json\"\n",
        "\n",
        "bbox_file(json_file_path, newjson_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaaHUgjSUzvG"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def obtain_bbox(path_to_file: str, name: str) -> list:\n",
        "    with open(path_to_file, 'r') as file:\n",
        "        bboxs = json.load(file)\n",
        "        return bboxs[name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtOTXBDdUzvG"
      },
      "outputs": [],
      "source": [
        "def crop_frame(bbox: list, frame):\n",
        "\n",
        "    cropped_frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
        "    return cropped_frame\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmofCWLyUzvG"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "def capture_frames(video_path, output_file, bbox):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if bbox:\n",
        "            frame = crop_frame(bbox, frame)\n",
        "\n",
        "        if frame_count >= 5 and frame_count <= 35:\n",
        "            frame_filename = f\"video{os.path.basename(video_path).replace('.mp4', '')}_frame_{frame_count}.jpg\"\n",
        "            cv2.imwrite(os.path.join(output_file, frame_filename), frame)\n",
        "        frame_count += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##if len(frames) < max_frames:\n",
        "        ##pad_frames = [frames[-1]] * (max_frames - len(frames))\n",
        "        ##frames.extend(pad_frames)\n",
        "\n",
        "\n",
        "    ##video_tensor = torch.stack(frames)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0H0iLwpUzvG"
      },
      "outputs": [],
      "source": [
        "bbox_path = \"drive/MyDrive/newfile.json\"\n",
        "videosfilepath = \"drive/MyDrive/dataset_split\"\n",
        "output_folder = \"drive/MyDrive/frames\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "splits = ['test','train','val']\n",
        "\n",
        "\n",
        "for split in splits:\n",
        "\n",
        "    files = [f for f in pathlib.Path(os.path.join(videosfilepath,split)).iterdir()]\n",
        "    for file in files:\n",
        "        i = 0\n",
        "        word = os.path.basename(file)\n",
        "\n",
        "        output_file = os.path.join(output_folder, split, word)\n",
        "\n",
        "        os.makedirs(output_file, exist_ok=True)\n",
        "\n",
        "        if file.is_dir():\n",
        "            videos = [f for f in file.iterdir()]\n",
        "        else:\n",
        "          print(f\"⚠️ Skipping non-directory: {file}\")\n",
        "          continue\n",
        "        for video_path in videos:\n",
        "\n",
        "            video_id = os.path.basename(video_path).replace(\".mp4\", \"\" )\n",
        "            print(video_id, video_path)\n",
        "            bbox = obtain_bbox(bbox_path, video_id)\n",
        "\n",
        "            try:\n",
        "                vid_output = os.path.join(output_file, f\"{i}\")\n",
        "                os.makedirs(vid_output, exist_ok=True)\n",
        "\n",
        "                capture_frames(video_path, vid_output, bbox)\n",
        "            except Exception as e:\n",
        "                print(bbox)\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NSEARK5UzvG"
      },
      "outputs": [],
      "source": [
        "class FrameSequencer(Dataset):\n",
        "    def __init__(self, root_dir, sample_size = 5, batch_size = 35, target_size=(224, 224), shuffle=True):\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.sample_size = sample_size\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_indicies = {clss: i for i, clss in enumerate(self.classes)}\n",
        "\n",
        "        self.samples = []\n",
        "\n",
        "        for cls in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls)\n",
        "            vids = os.listdir(cls_dir)\n",
        "            for i in vids:\n",
        "                frame_files = (sorted(\n",
        "                    [os.path.join(cls_dir, i, fname)\n",
        "                                    for fname in os.listdir(os.path.join(cls_dir, i)) if fname.lower().endswith('.jpg')]\n",
        "                                    ))\n",
        "\n",
        "\n",
        "                for j in range(0, len(frame_files) - self.sample_size + 1, self.sample_size):\n",
        "                    sequence = frame_files[j: j + self.sample_size]\n",
        "                    self.samples.append((sequence, self.class_indicies[cls]))\n",
        "\n",
        "\n",
        "        self.shuffler()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.samples) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        seq_paths, label = self.samples[index]\n",
        "        sequence_imgs = []\n",
        "\n",
        "        for fp in seq_paths:\n",
        "\n",
        "            img = cv2.imread(fp)\n",
        "            if img is None:\n",
        "                continue\n",
        "\n",
        "            img = cv2.resize(img, self.target_size)\n",
        "            img = np.transpose(img, (2, 0, 1))\n",
        "            sequence_imgs.append(img)\n",
        "\n",
        "        images = torch.from_numpy(np.array(sequence_imgs)).float()\n",
        "        label = torch.tensor(label).long()\n",
        "\n",
        "        return images, label\n",
        "\n",
        "\n",
        "    def shuffler(self):\n",
        "        if self.shuffle:\n",
        "            rnd.shuffle(self.samples)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeiX-yQ6UzvH"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_sample(g):\n",
        "        images, labels = g[0]\n",
        "\n",
        "        for i, sequence in enumerate(images):\n",
        "            classname = list(g.class_indicies.keys())[list(g.class_indicies.values()).index(labels[i])]\n",
        "            rows = 1\n",
        "            cols = len(sequence)\n",
        "            for j, image in enumerate(sequence):\n",
        "\n",
        "\n",
        "                plt.subplot(rows, cols, j + 1)\n",
        "                plt.imshow(cv2.cvtColor(image.numpy(), cv2.COLOR_BGR2RGB))\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(classname)\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYiC9FycUzvH"
      },
      "outputs": [],
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, feature_size=1280, hidden_size=256, num_classes=10, num_layers=1, device=\"cpu\"):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "        self.cnn = mobilenet.features.to(device)\n",
        "        self.lstm = nn.LSTM(input_size=feature_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, channels, height, width)\n",
        "        try:\n",
        "          batch_size, seq_len, C, H, W = x.shape\n",
        "          x = x.view(batch_size * seq_len, C, H, W)\n",
        "\n",
        "          with torch.no_grad():\n",
        "              features = self.cnn(x)\n",
        "              features = F.adaptive_avg_pool2d(features, (1, 1))\n",
        "\n",
        "          lstm_input = features.view(batch_size, seq_len, -1)\n",
        "\n",
        "          output, _ = self.lstm(lstm_input)\n",
        "          last_hidden = output[:, -1, :]\n",
        "          return self.fc(last_hidden)\n",
        "        except Exception as e:\n",
        "          print(f\"The error is {e}\\n The output of x.shape is: {x.shape} \\n\\n\\n X is: {x}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NvdIEr1OGYi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/train\", batch_size=5, sample_size=5)\n",
        "val_dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/val\", batch_size=5, sample_size=5)\n",
        "test_dataset = FrameSequencer(root_dir=r\"drive/MyDrive/frames/test\", batch_size=5, sample_size=5)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_M9pi9vqJ17"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2,\n",
        "                            pin_memory=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=128, num_workers=2,\n",
        "                          pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOQxJNHzKJlC"
      },
      "outputs": [],
      "source": [
        "model = CNN_LSTM(num_classes=len(dataset.classes), device=\"cuda\").to(\"cuda\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft7P5ow6Phde"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
        "\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_model_state = None\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "\n",
        "    def __call__(self, model, validation_loss):\n",
        "        if validation_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = validation_loss\n",
        "            self.counter = 0\n",
        "            if self.restore_best_weights:\n",
        "                self.best_model_state = deepcopy(model.state_dict())\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "    def restore_model(self, model):\n",
        "        if self.best_model_state is not None:\n",
        "            model.load_state_dict(self.best_model_state)\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rORXgDVQQCpL"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            losses.append(loss.item())\n",
        "    return sum(losses) / len(losses)\n",
        "\n",
        "def train_one_epoch(model, train_loader, criterion, optimizer, device=\"cpu\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_x, batch_y in tqdm(train_loader, desc=\"Training\", leave=False):\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def training_loop(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    early_stopper=None,\n",
        "    epochs=10,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        val_loss = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopper:\n",
        "            early_stopper(model, val_loss)\n",
        "            if early_stopper.early_stop:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Restore best model if early stopping was used\n",
        "    if early_stopper and early_stopper.restore_best_weights:\n",
        "        model = early_stopper.restore_model(model)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwBiQSC7XbGs"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgKPT33CazrL",
        "outputId": "bd65ae00-227e-4e6b-9c5d-d812de7e21fd"
      },
      "outputs": [],
      "source": [
        "model = training_loop(\n",
        "    model=model,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    early_stopper=early_stopping,\n",
        "    epochs=10,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOEOfciUbaSw"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"asl_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
