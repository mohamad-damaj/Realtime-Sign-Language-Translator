{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['kaggle', 'datasets', 'download', '-d', 'risangbaskoro/wlasl-processed', '-p', 'C:\\\\Users\\\\user\\\\Desktop\\\\Sign Language Translator\\\\artifact'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "os.makedirs(r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\", exist_ok = True)\n",
    "command = [\n",
    "    \"kaggle\",\n",
    "    \"datasets\",\n",
    "    \"download\",\n",
    "    \"-d\",\n",
    "    \"risangbaskoro/wlasl-processed\",\n",
    "    \"-p\",\n",
    "    r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\"\n",
    "]\n",
    "\n",
    "subprocess.run(command, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\wlasl-processed.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "\n",
    "def form_dataset():\n",
    "    os.makedirs(r\".\\artifact\\train\", exist_ok = True)\n",
    "    os.makedirs(r\".\\artifact\\test\", exist_ok = True)\n",
    "    files = [f for f in pathlib.Path(r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\asl_dataset\").iterdir()]\n",
    "    for folder in files:\n",
    "     \n",
    "        images = [f for f in pathlib.Path(folder).iterdir()]\n",
    "        i = 0\n",
    "        for image in images:\n",
    "            if i % 3 == 0:\n",
    "                shutil.move(image, r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\test\")\n",
    "            else: \n",
    "                shutil.move(image, r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\train\")\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "\n",
    "form_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\artifact\\\\WLASL_v0.3.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Opening the json\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     15\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gloss_data \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\user\\Desktop\\Sign Language Translator\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\artifact\\\\WLASL_v0.3.json'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Paths\n",
    "json_file_path = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\WLASL_v0.3.json\"  # Replace with your JSON file\n",
    "videos_folder = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\videos\"         # Folder containing videos named by video_id\n",
    "output_folder = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\dataset_split\"  # Root folder for train/test/val splits\n",
    "newjson_file_path = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\newfile.json\"\n",
    "\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "# Opening the json\n",
    "with open(json_file_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "for gloss_data in data:\n",
    "    gloss_name = gloss_data[\"gloss\"]  \n",
    "    \n",
    "    for instance in gloss_data[\"instances\"]:\n",
    "        video_id = instance[\"video_id\"]  \n",
    "        split = instance[\"split\"]        \n",
    "        \n",
    "        source_path = os.path.join(videos_folder, f\"{video_id}.mp4\")\n",
    "        dest_folder = os.path.join(output_folder, split, gloss_name)\n",
    "        dest_path = os.path.join(dest_folder, f\"{video_id}.mp4\")\n",
    "        \n",
    "        # Ensuring that destination folder exists\n",
    "        os.makedirs(dest_folder, exist_ok=True)\n",
    "        \n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy(source_path, dest_path)\n",
    "            print(f\"Copied {source_path} to {dest_path}\")\n",
    "        else:\n",
    "            print(f\"Video not found: {source_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "json_file_path = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\WLASL_v0.3.json\"  \n",
    "newjson_file_path = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\newfile.json\"\n",
    "\n",
    "def bbox_file(info, output_json_path):\n",
    "    with open(info, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    id_to_bbox = {}\n",
    "    for gloss_data in data:\n",
    "        for instance in gloss_data[\"instances\"]:\n",
    "            video_id = instance[\"video_id\"]\n",
    "            boundbox = instance[\"bbox\"]\n",
    "            id_to_bbox[video_id] = boundbox\n",
    "\n",
    "\n",
    "    with open(output_json_path, \"w\") as file:\n",
    "        json.dump(id_to_bbox, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def obtain_bbox(path_to_file: str, name: str) -> list:\n",
    "    with open(path_to_file, 'r') as file:\n",
    "        bboxs = json.load(file)\n",
    "        return bboxs[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_frame(bbox: list, frame):\n",
    "\n",
    "    cropped_frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2]]\n",
    "    return cropped_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "def capture_frames(video_path, bbox, output_file):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if bbox:\n",
    "            frame = crop_frame(bbox, frame)\n",
    "\n",
    "        frame_filename = f\"video{os.path.basename(file).replace('.mp4', '')}_frame_{frame_count}.jpg\"\n",
    "        cv2.imwrite(os.path.join(output_file, frame_filename), frame)\n",
    "        frame_count += 1 \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##if len(frames) < max_frames:\n",
    "        ##pad_frames = [frames[-1]] * (max_frames - len(frames))\n",
    "        ##frames.extend(pad_frames)\n",
    "\n",
    "\n",
    "    ##video_tensor = torch.stack(frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66039 C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\dataset_split\\test\\a\\66039.mp4\n",
      "[167, 13, 485, 370]\n",
      ":\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m bbox \u001b[38;5;241m=\u001b[39m obtain_bbox(bbox_path, video_id)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(bbox)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mcapture_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mcapture_frames\u001b[1;34m(video_path, bbox, output_file)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox:\n\u001b[1;32m---> 11\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m frame_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_file, frame_filename), frame)\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mcrop_frame\u001b[1;34m(bbox, frame)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcrop_frame\u001b[39m(bbox: \u001b[38;5;28mlist\u001b[39m, frame):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(bbox[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     cropped_frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cropped_frame\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import cv2\n",
    "\n",
    "bbox_path = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\newfile.json\"\n",
    "videosfilepath = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\dataset_split\"\n",
    "output_folder = r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\frames\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "splits = ['test','train','val']\n",
    "\n",
    "\n",
    "for split in splits:\n",
    "\n",
    "    files = [f for f in pathlib.Path(os.path.join(videosfilepath,split)).iterdir()]\n",
    "    for file in files:\n",
    "        word = os.path.basename(file) # extracting word name\n",
    "\n",
    "        output_file = os.path.join(output_folder, split, word)\n",
    "\n",
    "        os.makedirs(output_file, exist_ok=True) \n",
    "        \n",
    "        videos = [f for f in pathlib.Path(file).iterdir()]\n",
    "\n",
    "        for video_path in videos:\n",
    "            \n",
    "            video_id = os.path.basename(video_path).replace(\".mp4\", \"\" )\n",
    "            print(video_id, video_path)\n",
    "            bbox = obtain_bbox(bbox_path, video_id)\n",
    "            capture_frames(video_path, output_file, bbox)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m3,078\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,717,766</span> (56.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,717,766\u001b[0m (56.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> (12.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,078\u001b[0m (12.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, try to augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = ImageDataGenerator(    samplewise_center=True,  # set each sample mean to 0\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True,\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(samplewise_center=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created). For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 36 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen_train.flow_from_directory(\n",
    "    r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\train\",\n",
    "    target_size=(224,224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen_valid.flow_from_directory(\n",
    "    r\"C:\\Users\\user\\Desktop\\Sign Language Translator\\artifact\\test\",\n",
    "    target_size=(224,224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting the desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "37/36 [==============================] - 26s 694ms/step - loss: 2.5725 - accuracy: 0.4239 - val_loss: 1.0127 - val_accuracy: 0.6991\n",
      "Epoch 2/9\n",
      "37/36 [==============================] - 17s 470ms/step - loss: 0.7845 - accuracy: 0.7352 - val_loss: 0.4406 - val_accuracy: 0.8693\n",
      "Epoch 3/9\n",
      "37/36 [==============================] - 17s 466ms/step - loss: 0.4457 - accuracy: 0.8418 - val_loss: 0.3310 - val_accuracy: 0.8875\n",
      "Epoch 4/9\n",
      "37/36 [==============================] - 17s 457ms/step - loss: 0.2950 - accuracy: 0.8942 - val_loss: 0.2288 - val_accuracy: 0.9301\n",
      "Epoch 5/9\n",
      "37/36 [==============================] - 17s 457ms/step - loss: 0.2355 - accuracy: 0.9146 - val_loss: 0.1970 - val_accuracy: 0.9392\n",
      "Epoch 6/9\n",
      "37/36 [==============================] - 17s 462ms/step - loss: 0.1811 - accuracy: 0.9340 - val_loss: 0.2193 - val_accuracy: 0.9149\n",
      "Epoch 7/9\n",
      "37/36 [==============================] - 17s 464ms/step - loss: 0.1280 - accuracy: 0.9442 - val_loss: 0.1507 - val_accuracy: 0.9453\n",
      "Epoch 8/9\n",
      "37/36 [==============================] - 17s 460ms/step - loss: 0.1307 - accuracy: 0.9535 - val_loss: 0.1465 - val_accuracy: 0.9544\n",
      "Epoch 9/9\n",
      "37/36 [==============================] - 17s 470ms/step - loss: 0.1127 - accuracy: 0.9594 - val_loss: 0.1620 - val_accuracy: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd0c02ce48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = 0.00001),\n",
    "              loss = 'categorical_crossentropy' , metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/36 [==============================] - 31s 827ms/step - loss: 0.1083 - accuracy: 0.9611 - val_loss: 0.1446 - val_accuracy: 0.9666\n",
      "Epoch 2/10\n",
      "37/36 [==============================] - 19s 507ms/step - loss: 0.0825 - accuracy: 0.9695 - val_loss: 0.1075 - val_accuracy: 0.9726\n",
      "Epoch 3/10\n",
      "37/36 [==============================] - 18s 477ms/step - loss: 0.0472 - accuracy: 0.9873 - val_loss: 0.1595 - val_accuracy: 0.9666\n",
      "Epoch 4/10\n",
      "37/36 [==============================] - 18s 485ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.1698 - val_accuracy: 0.9666\n",
      "Epoch 5/10\n",
      "37/36 [==============================] - 18s 487ms/step - loss: 0.0529 - accuracy: 0.9822 - val_loss: 0.0960 - val_accuracy: 0.9757\n",
      "Epoch 6/10\n",
      "37/36 [==============================] - 19s 504ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.1878 - val_accuracy: 0.9696\n",
      "Epoch 7/10\n",
      "37/36 [==============================] - 18s 482ms/step - loss: 0.0274 - accuracy: 0.9924 - val_loss: 0.1136 - val_accuracy: 0.9666\n",
      "Epoch 8/10\n",
      "37/36 [==============================] - 18s 492ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0533 - val_accuracy: 0.9818\n",
      "Epoch 9/10\n",
      "37/36 [==============================] - 18s 487ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0723 - val_accuracy: 0.9818\n",
      "Epoch 10/10\n",
      "31/36 [========================>.....] - ETA: 2s - loss: 0.0245 - accuracy: 0.9939"
     ]
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
