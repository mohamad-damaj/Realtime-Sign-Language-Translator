{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3333904223.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    path = kagglehub.dataset_download(\"ayuraj/asl-dataset\", download_path=\"C:\\Users\\user\\Desktop\\Sign Language Translator\")\u001b[0m\n\u001b[1;37m                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "path = kagglehub.dataset_download(\"ayuraj/asl-dataset\", download_path=\"C:\\Users\\user\\Desktop\\Sign Language Translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\.cache\\\\kagglehub\\\\datasets\\\\ayuraj\\\\asl-dataset\\\\versions\\\\1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Layers to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs with correct shape\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "\n",
    "# Add pooling layer or flatten layer\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add final dense layer\n",
    "outputs = keras.layers.Dense(6, activation = 'softmax')(x)\n",
    "\n",
    "# Combine inputs and outputs to create model\n",
    "model = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 14,717,766\n",
      "Trainable params: 3,078\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. Remember that we're training on a number of different categories, rather than a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like, try to augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [Keras ImageDataGenerator class](https://keras.io/api/preprocessing/image/#imagedatagenerator-class). This step is optional, but it may be helpful to get to 92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen_train = ImageDataGenerator(    samplewise_center=True,  # set each sample mean to 0\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True,\n",
    ")\n",
    "datagen_valid = ImageDataGenerator(samplewise_center=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. Pick the right folders, as well as the right `target_size` of the images (it needs to match the height and width input of the model you've created). For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1182 images belonging to 6 classes.\n",
      "Found 329 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "# load and iterate training dataset\n",
    "train_it = datagen_train.flow_from_directory(\n",
    "    \"data/fruits/train/\",\n",
    "    target_size=(224,224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "# load and iterate validation dataset\n",
    "valid_it = datagen_valid.flow_from_directory(\n",
    "    \"data/fruits/valid/\",\n",
    "    target_size=(224,224),\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! Pass the `train` and `valid` iterators into the `fit` function, as well as setting the desired number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "37/36 [==============================] - 26s 694ms/step - loss: 2.5725 - accuracy: 0.4239 - val_loss: 1.0127 - val_accuracy: 0.6991\n",
      "Epoch 2/9\n",
      "37/36 [==============================] - 17s 470ms/step - loss: 0.7845 - accuracy: 0.7352 - val_loss: 0.4406 - val_accuracy: 0.8693\n",
      "Epoch 3/9\n",
      "37/36 [==============================] - 17s 466ms/step - loss: 0.4457 - accuracy: 0.8418 - val_loss: 0.3310 - val_accuracy: 0.8875\n",
      "Epoch 4/9\n",
      "37/36 [==============================] - 17s 457ms/step - loss: 0.2950 - accuracy: 0.8942 - val_loss: 0.2288 - val_accuracy: 0.9301\n",
      "Epoch 5/9\n",
      "37/36 [==============================] - 17s 457ms/step - loss: 0.2355 - accuracy: 0.9146 - val_loss: 0.1970 - val_accuracy: 0.9392\n",
      "Epoch 6/9\n",
      "37/36 [==============================] - 17s 462ms/step - loss: 0.1811 - accuracy: 0.9340 - val_loss: 0.2193 - val_accuracy: 0.9149\n",
      "Epoch 7/9\n",
      "37/36 [==============================] - 17s 464ms/step - loss: 0.1280 - accuracy: 0.9442 - val_loss: 0.1507 - val_accuracy: 0.9453\n",
      "Epoch 8/9\n",
      "37/36 [==============================] - 17s 460ms/step - loss: 0.1307 - accuracy: 0.9535 - val_loss: 0.1465 - val_accuracy: 0.9544\n",
      "Epoch 9/9\n",
      "37/36 [==============================] - 17s 470ms/step - loss: 0.1127 - accuracy: 0.9594 - val_loss: 0.1620 - val_accuracy: 0.9392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbd0c02ce48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Compile the model with a low learning rate\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate = 0.00001),\n",
    "              loss = 'categorical_crossentropy' , metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/36 [==============================] - 31s 827ms/step - loss: 0.1083 - accuracy: 0.9611 - val_loss: 0.1446 - val_accuracy: 0.9666\n",
      "Epoch 2/10\n",
      "37/36 [==============================] - 19s 507ms/step - loss: 0.0825 - accuracy: 0.9695 - val_loss: 0.1075 - val_accuracy: 0.9726\n",
      "Epoch 3/10\n",
      "37/36 [==============================] - 18s 477ms/step - loss: 0.0472 - accuracy: 0.9873 - val_loss: 0.1595 - val_accuracy: 0.9666\n",
      "Epoch 4/10\n",
      "37/36 [==============================] - 18s 485ms/step - loss: 0.0353 - accuracy: 0.9873 - val_loss: 0.1698 - val_accuracy: 0.9666\n",
      "Epoch 5/10\n",
      "37/36 [==============================] - 18s 487ms/step - loss: 0.0529 - accuracy: 0.9822 - val_loss: 0.0960 - val_accuracy: 0.9757\n",
      "Epoch 6/10\n",
      "37/36 [==============================] - 19s 504ms/step - loss: 0.0216 - accuracy: 0.9932 - val_loss: 0.1878 - val_accuracy: 0.9696\n",
      "Epoch 7/10\n",
      "37/36 [==============================] - 18s 482ms/step - loss: 0.0274 - accuracy: 0.9924 - val_loss: 0.1136 - val_accuracy: 0.9666\n",
      "Epoch 8/10\n",
      "37/36 [==============================] - 18s 492ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0533 - val_accuracy: 0.9818\n",
      "Epoch 9/10\n",
      "37/36 [==============================] - 18s 487ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0723 - val_accuracy: 0.9818\n",
      "Epoch 10/10\n",
      "31/36 [========================>.....] - ETA: 2s - loss: 0.0245 - accuracy: 0.9939"
     ]
    }
   ],
   "source": [
    "model.fit(train_it,\n",
    "          validation_data=valid_it,\n",
    "          steps_per_epoch=train_it.samples/train_it.batch_size,\n",
    "          validation_steps=valid_it.samples/valid_it.batch_size,\n",
    "          epochs=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(valid_it, steps=valid_it.samples/valid_it.batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
